{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38ca805",
   "metadata": {},
   "outputs": [],
   "source": [
    "#**Задача 1: Система управления библиотекой**\n",
    "\n",
    "from collections import namedtuple\n",
    "from collections import defaultdict\n",
    "from typing import NamedTuple\n",
    "\n",
    "Book = namedtuple('Book', ['title', 'author', 'isbn', 'year'])\n",
    "books = []\n",
    "Reader = namedtuple('Reader', ['name', 'reader_id', 'phone'])\n",
    "readers = []\n",
    "\n",
    "# создаю словарь с жанрами и добавляю туда книги\n",
    "genre_book = defaultdict(list)\n",
    "\n",
    "genre_book[genre].append(Book(title, author, isbn, year))\n",
    "\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "# cоздаю словарь из id как ключа и Reader как значения для более удобного хранения и доступа\n",
    "readers_by_id = {r.reader_id: r for r in readers}\n",
    "\n",
    "# делаю очередь\n",
    "book_queue = deque()\n",
    "_ = [book_queue.append(reader_id)]\n",
    "\n",
    "# считаю\n",
    "from collections import Counter\n",
    "\n",
    "all_authors = [book.author for book in books]\n",
    "author_count = Counter(all_authors)\n",
    "\n",
    "# история\n",
    "\n",
    "book_history = OrderedDict()\n",
    "\n",
    "def add_to_history(reader_id, book):\n",
    "    if reader_id in book_history:\n",
    "        book_history[reader_id].append(book)\n",
    "    else:\n",
    "        book_history[reader_id] = [book]\n",
    "\n",
    "# подготовка к сериализации: namedtuple нужно преобразовать в словари\n",
    "def serialize_namedtuple(obj):\n",
    "    if isinstance(obj, (Book, Reader)):\n",
    "        return obj._asdict()\n",
    "    if isinstance(obj, list):\n",
    "        return [serialize_namedtuple(i) for i in obj]\n",
    "    return obj\n",
    "\n",
    "data = {\n",
    "    'books': serialize_namedtuple(books),\n",
    "    'readers': serialize_namedtuple(readers),\n",
    "    'genre_book': {genre: serialize_namedtuple(book_list) for genre, book_list in genre_book.items()},\n",
    "    'book_queue': list(book_queue),\n",
    "    'author_count': dict(author_count),\n",
    "    'book_history': {reader_id: serialize_namedtuple(book_list) for reader_id, book_list in book_history.items()},\n",
    "}\n",
    "\n",
    "# JSON сериализация\n",
    "with open('library_data.json', 'w', encoding='utf-8') as f_json:\n",
    "    json.dump(data, f_json, ensure_ascii=False, indent=4)\n",
    "\n",
    "# Pickle сериализация (можно сериализовать оригинальные объекты)\n",
    "with open('library_data.pkl', 'wb') as f_pickle:\n",
    "    pickle.dump({\n",
    "        'books': books,\n",
    "        'readers': readers,\n",
    "        'genre_book': genre_book,\n",
    "        'book_queue': book_queue,\n",
    "        'author_count': author_count,\n",
    "        'book_history': book_history,\n",
    "    }, f_pickle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780559e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#**Задача 2: Анализатор файловой системы**\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "from collections import namedtuple, Counter, defaultdict, deque\n",
    "\n",
    "#  создаем namedtuple для хранения информации о файле\n",
    "FileInfo = namedtuple('FileInfo', ['name', 'size', 'extension', 'modified_time'])\n",
    "\n",
    "#  классификатор по размеру\n",
    "def classify_size(size_bytes):\n",
    "    if size_bytes < 1 * 1024 * 1024:\n",
    "        return 'small'\n",
    "    elif 1 * 1024 * 1024 <= size_bytes <= 100 * 1024 * 1024:\n",
    "        return 'medium'\n",
    "    else:\n",
    "        return 'large'\n",
    "\n",
    "#  основная функция анализатора\n",
    "def analyze_directory(root_path):\n",
    "    extension_counter = Counter()\n",
    "    size_groups = defaultdict(list)\n",
    "    recent_files = deque(maxlen=10)\n",
    "    all_files_info = []\n",
    "\n",
    "    for dirpath, dirnames, filenames in os.walk(root_path):\n",
    "        for filename in filenames:\n",
    "            full_path = os.path.join(dirpath, filename)\n",
    "            try:\n",
    "                # информация о файле\n",
    "                size = os.path.getsize(full_path)\n",
    "                extension = os.path.splitext(filename)[1].lower()\n",
    "                modified_timestamp = os.path.getmtime(full_path)\n",
    "                modified_time = os.path.getmtime(full_path)\n",
    "                \n",
    "                file_info = FileInfo(\n",
    "                    name=full_path,\n",
    "                    size=size,\n",
    "                    extension=extension,\n",
    "                    modified_time=modified_time\n",
    "                )\n",
    "\n",
    "                # собираем данные\n",
    "                all_files_info.append(file_info)\n",
    "                extension_counter[extension] += 1\n",
    "                size_category = classify_size(size)\n",
    "                size_groups[size_category].append(file_info)\n",
    "                recent_files.append(file_info)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Ошибка при обработке файла {full_path}: {e}\")\n",
    "\n",
    "    # 7. статистика по памяти\n",
    "    memory_usage = {\n",
    "        'all_files_info': sys.getsizeof(all_files_info),\n",
    "        'extension_counter': sys.getsizeof(extension_counter),\n",
    "        'size_groups': sys.getsizeof(size_groups),\n",
    "        'recent_files': sys.getsizeof(recent_files)\n",
    "    }\n",
    "\n",
    "    # 8. сохраняем в JSON\n",
    "    result = {\n",
    "        'extensions': dict(extension_counter),\n",
    "        'size_groups': {\n",
    "            key: [file._asdict() for file in files] for key, files in size_groups.items()\n",
    "        },\n",
    "        'recent_files': [file._asdict() for file in recent_files],\n",
    "        'memory_usage': memory_usage\n",
    "    }\n",
    "\n",
    "    with open('filesystem_analysis.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(result, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(\"Анализ завершен. Результаты сохранены в 'filesystem_analysis.json'.\")\n",
    "\n",
    "# точка входа\n",
    "if __name__ == '__main__':\n",
    "    if len(sys.argv) != 2:\n",
    "        print(\"Использование: python analyzer.py <путь_к_директории>\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    directory_path = sys.argv[1]\n",
    "    if not os.path.isdir(directory_path):\n",
    "        print(\"Указанный путь не является директорией.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    analyze_directory(directory_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f36a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#**Задача 3: Система конфигурации приложения**\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "from collections import namedtuple, ChainMap, defaultdict, OrderedDict\n",
    "\n",
    "# создание namedtuple `Config`\n",
    "Config = namedtuple(\"Config\", [\"key\", \"value\", \"section\", \"default_value\"])\n",
    "\n",
    "# словари конфигураций\n",
    "default_config = {\n",
    "    \"host\": \"localhost\",\n",
    "    \"port\": 8080,\n",
    "    \"debug\": False,\n",
    "    \"log_level\": \"INFO\"\n",
    "}\n",
    "\n",
    "user_config = {\n",
    "    \"debug\": True,\n",
    "    \"log_level\": \"DEBUG\"\n",
    "}\n",
    "\n",
    "# test (добавим переменные окружения в os.environ)\n",
    "os.environ[\"host\"] = \"prod.server.com\"\n",
    "os.environ[\"port\"] = \"9090\"\n",
    "\n",
    "# преобразуем переменные окружения в словарь (преобразуя значения к нужным типам)\n",
    "env_config = {\n",
    "    \"host\": os.environ.get(\"host\"),\n",
    "    \"port\": int(os.environ.get(\"port\", 8080))\n",
    "}\n",
    "\n",
    "# объединение конфигураций с приоритетом\n",
    "combined_config = ChainMap(env_config, user_config, default_config)\n",
    "\n",
    "# группировка настроек по секциям\n",
    "section_mapping = {\n",
    "    \"host\": \"network\",\n",
    "    \"port\": \"network\",\n",
    "    \"debug\": \"app\",\n",
    "    \"log_level\": \"app\"\n",
    "}\n",
    "\n",
    "grouped_config = defaultdict(dict)\n",
    "ordered_config = OrderedDict()\n",
    "\n",
    "for key in combined_config:\n",
    "    section = section_mapping.get(key, \"other\")\n",
    "    value = combined_config[key]\n",
    "    default_value = default_config.get(key)\n",
    "    \n",
    "    config_entry = Config(\n",
    "        key=key,\n",
    "        value=value,\n",
    "        section=section,\n",
    "        default_value=default_value\n",
    "    )\n",
    "\n",
    "    grouped_config[section][key] = config_entry\n",
    "    ordered_config[key] = config_entry\n",
    "\n",
    "# сериализация в JSON и pickle\n",
    "# подготовим словарь для сериализации (нельзя сериализовать namedtuple напрямую в JSON)\n",
    "serializable_config = {\n",
    "    key: {\n",
    "        \"value\": config.value,\n",
    "        \"section\": config.section,\n",
    "        \"default_value\": config.default_value\n",
    "    }\n",
    "    for key, config in ordered_config.items()\n",
    "}\n",
    "\n",
    "# JSON\n",
    "with open(\"config.json\", \"w\") as json_file:\n",
    "    json.dump(serializable_config, json_file, indent=4)\n",
    "\n",
    "# Pickle\n",
    "with open(\"config.pkl\", \"wb\") as pickle_file:\n",
    "    pickle.dump(ordered_config, pickle_file)\n",
    "\n",
    "# вывод grouped_config (по секциям)\n",
    "for section, configs in grouped_config.items():\n",
    "    print(f\"[{section.upper()}]\")\n",
    "    for key, config in configs.items():\n",
    "        print(f\"{key} = {config.value} (default: {config.default_value})\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de7fc2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#**Задача 4: Мониторинг системы**\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pickle\n",
    "from collections import namedtuple, deque, Counter, defaultdict\n",
    "from datetime import datetime\n",
    "\n",
    "# создаем namedtuple\n",
    "SystemInfo = namedtuple('SystemInfo', ['cpu_count', 'memory_usage', 'process_id', 'user_name'])\n",
    "\n",
    "# хранилище последних 20 измерений\n",
    "history = deque(maxlen=20)\n",
    "\n",
    "# счетчик использования функций\n",
    "function_usage = Counter()\n",
    "\n",
    "# группировка измерений по времени\n",
    "grouped_by_time = defaultdict(list)\n",
    "\n",
    "# функция мониторинга\n",
    "def collect_system_info():\n",
    "    function_usage['collect_system_info'] += 1\n",
    "\n",
    "    cpu_count = os.cpu_count()\n",
    "    memory_usage = sys.getallocatedblocks()\n",
    "    process_id = os.getpid()\n",
    "    \n",
    "    try:\n",
    "        user_name = os.getlogin()\n",
    "    except OSError:\n",
    "        user_name = os.environ.get('USERNAME') or os.environ.get('USER') or 'unknown'\n",
    "\n",
    "    info = SystemInfo(cpu_count, memory_usage, process_id, user_name)\n",
    "    return info\n",
    "\n",
    "# основной цикл мониторинга\n",
    "def monitor_system(interval=2, duration=20):\n",
    "    for _ in range(duration):\n",
    "        info = collect_system_info()\n",
    "        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        \n",
    "        # добавляем в историю\n",
    "        history.append(info)\n",
    "\n",
    "        # группируем по времени\n",
    "        grouped_by_time[timestamp].append(info)\n",
    "\n",
    "        print(f\"[{timestamp}] {info}\")\n",
    "        time.sleep(interval)\n",
    "\n",
    "# сохраняем историю в pickle\n",
    "def save_history(filename='monitoring_history.pkl'):\n",
    "    function_usage['save_history'] += 1\n",
    "\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump({\n",
    "            'history': list(history),\n",
    "            'grouped_by_time': dict(grouped_by_time),\n",
    "            'function_usage': dict(function_usage)\n",
    "        }, f)\n",
    "    print(f\"История мониторинга сохранена в {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e29f9c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LogEntry(timestamp=1758466828.7362828, level='INFO', message='System started', module='main', function='start')]\n",
      "Counter({'INFO': 1})\n"
     ]
    }
   ],
   "source": [
    "#**Задача 5: Система логирования**\n",
    "\n",
    "from collections import namedtuple, deque, defaultdict, Counter, OrderedDict, ChainMap\n",
    "import os\n",
    "\n",
    "LogEntry = namedtuple('LogEntry', ['timestamp', 'level', 'message', 'module', 'function'])\n",
    "\n",
    "# test\n",
    "log_entry = LogEntry(\n",
    "    timestamp=os.path.getmtime('.'),\n",
    "    level='INFO',\n",
    "    message='System started',\n",
    "    module='main',\n",
    "    function='start'\n",
    ")\n",
    "\n",
    "history_logs = deque(maxlen=100)\n",
    "history_logs.appendleft(log_entry)  \n",
    "\n",
    "grouped_logs = defaultdict(list)\n",
    "grouped_logs[log_entry.level].append(log_entry)\n",
    "print(grouped_logs['INFO'])\n",
    "\n",
    "logs_count = Counter({level: len(logs) for level, logs in grouped_logs.items()})\n",
    "print(logs_count)\n",
    "\n",
    "time_logs = OrderedDict()\n",
    "time_logs[log_entry.timestamp] = log_entry\n",
    "time_logs.popitem(last=False)\n",
    "\n",
    "all_logs = ChainMap(grouped_logs, logs_count, time_logs)\n",
    "\n",
    "# сериализация\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "# функция для того чтобы можно было логи сохранить в json'е\n",
    "def log_entry_to_dict(log_entry):\n",
    "    return {\n",
    "        'timestamp': log_entry.timestamp,\n",
    "        'level': log_entry.level,\n",
    "        'message': log_entry.message,\n",
    "        'module': log_entry.module,\n",
    "        'function': log_entry.function\n",
    "    }\n",
    "\n",
    "# сохраняем history_logs в JSON\n",
    "with open('logs.json', 'w', encoding='utf-8') as f_json:\n",
    "    json.dump([log_entry_to_dict(log) for log in history_logs], f_json, ensure_ascii=False, indent=4)\n",
    "\n",
    "# сохраняем history_logs в pickle\n",
    "with open('logs.pkl', 'wb') as f_pickle:\n",
    "    pickle.dump(history_logs, f_pickle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991d0fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#**Задача 6: Кэш-система**\n",
    "\n",
    "from collections import namedtuple, deque, defaultdict, Counter, OrderedDict, ChainMap\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "\n",
    "CacheEntry = namedtuple('CacheEntry', ['key', 'value', 'timestamp', 'access_count'])\n",
    "\n",
    "# test\n",
    "cache_test = CacheEntry(\n",
    "    key='key',\n",
    "    value='value',\n",
    "    timestamp=0,\n",
    "    access_count=0\n",
    ")\n",
    "\n",
    "cache_LRU = OrderedDict()\n",
    "access_history = deque()\n",
    "access_counter = Counter()\n",
    "access_stats = defaultdict(int)\n",
    "MAX_CACHE_SIZE = 5\n",
    "\n",
    "def set(key, value):\n",
    "    global cache_LRU, access_history, access_counter, access_stats\n",
    "\n",
    "    # получить временную метку файла, если он существует\n",
    "    try:\n",
    "        timestamp = os.path.getmtime(key)\n",
    "    except FileNotFoundError:\n",
    "        timestamp = 0 \n",
    "\n",
    "    # если ключ есть, то обновляем значение и увеличиваем счетчик обращений\n",
    "    if key in cache_LRU:\n",
    "        old_entry = cache_LRU[key]\n",
    "        new_entry = CacheEntry(\n",
    "            key=key,\n",
    "            value=value,\n",
    "            timestamp=timestamp,\n",
    "            access_count=old_entry.access_count + 1\n",
    "        )\n",
    "        # двигаем в конец\n",
    "        cache_LRU.move_to_end(key)\n",
    "        cache_LRU[key] = new_entry\n",
    "    else:\n",
    "        # удаление если много ключей\n",
    "        if len(cache_LRU) >= MAX_CACHE_SIZE:\n",
    "            cache_LRU.popitem(last=False)\n",
    "\n",
    "        new_entry = CacheEntry(\n",
    "            key=key,\n",
    "            value=value,\n",
    "            timestamp=timestamp,\n",
    "            access_count=1\n",
    "        )\n",
    "        cache_LRU[key] = new_entry\n",
    "\n",
    "    # статистика\n",
    "    access_history.append(key)\n",
    "    access_counter[key] += 1\n",
    "    access_stats[key] += 1\n",
    "\n",
    "def get(key):\n",
    "    global cache_LRU, access_history, access_counter, access_stats\n",
    "\n",
    "    # если ключа нет в кэше — возвращаем None\n",
    "    if key not in cache_LRU:\n",
    "        return None\n",
    "\n",
    "    # обновляем порядок\n",
    "    cache_LRU.move_to_end(key)\n",
    "\n",
    "    # увеличиваем счетчик обращений\n",
    "    old_entry = cache_LRU[key]\n",
    "    new_entry = CacheEntry(\n",
    "        key=key,\n",
    "        value=old_entry.value,\n",
    "        timestamp=old_entry.timestamp,\n",
    "        access_count=old_entry.access_count + 1\n",
    "    )\n",
    "    cache_LRU[key] = new_entry\n",
    "\n",
    "    # статистика\n",
    "    access_history.append(key)\n",
    "    access_counter[key] += 1\n",
    "    access_stats[key] += 1\n",
    "\n",
    "    return new_entry.value \n",
    "\n",
    "def delete_cache(key):\n",
    "    global cache_LRU, access_counter, access_stats\n",
    "\n",
    "    if key in cache_LRU:\n",
    "        del cache_LRU[key]\n",
    "        if key in access_counter:\n",
    "            del access_counter[key]\n",
    "        if key in access_stats:\n",
    "            del access_stats[key]\n",
    "\n",
    "# удаление кэша и всей статистики\n",
    "def clear():\n",
    "    global cache_LRU, access_history, access_counter, access_stats\n",
    "\n",
    "    cache_LRU.clear()\n",
    "    access_history.clear()\n",
    "    access_counter.clear()\n",
    "    access_stats.clear()\n",
    "\n",
    "def size():\n",
    "    total_size = sys.getsizeof(cache_LRU)\n",
    "    for entry in cache_LRU.values():\n",
    "        total_size += sys.getsizeof(entry)\n",
    "        total_size += sys.getsizeof(entry.key)\n",
    "        total_size += sys.getsizeof(entry.value)\n",
    "    return total_size\n",
    "\n",
    "def save(filepath):\n",
    "    global cache_LRU, access_history, access_counter, access_stats\n",
    "\n",
    "    with open(filepath, 'wb') as f:\n",
    "        pickle.dump({\n",
    "            'cache_LRU': cache_LRU,\n",
    "            'access_history': access_history,\n",
    "            'access_counter': access_counter,\n",
    "            'access_stats': access_stats,\n",
    "            'MAX_CACHE_SIZE': MAX_CACHE_SIZE\n",
    "        }, f)\n",
    "\n",
    "def load(filepath):\n",
    "    global cache_LRU, access_history, access_counter, access_stats, MAX_CACHE_SIZE\n",
    "\n",
    "    with open(filepath, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "        cache_LRU = data.get('cache_LRU', OrderedDict())\n",
    "        access_history = data.get('access_history', deque())\n",
    "        access_counter = data.get('access_counter', Counter())\n",
    "        access_stats = data.get('access_stats', defaultdict(int))\n",
    "        MAX_CACHE_SIZE = data.get('MAX_CACHE_SIZE', 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a222bb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#**Задача 7: Анализатор текста**\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "from collections import namedtuple, Counter, defaultdict, deque, OrderedDict\n",
    "\n",
    "# 1. создаем namedtuple\n",
    "WordInfo = namedtuple('WordInfo', ['word', 'frequency', 'length', 'first_occurrence'])\n",
    "\n",
    "def analyze_text(file_path):\n",
    "    if not os.path.isfile(file_path):\n",
    "        raise FileNotFoundError(f\"Файл '{file_path}' не найден.\")\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "\n",
    "    words = [word.strip(\".,!?;:()[]{}\\\"'\").lower() for word in text.split()]\n",
    "    \n",
    "    # 2. подсчет частоты слов\n",
    "    word_counter = Counter(words)\n",
    "\n",
    "    # 3. группируем слова по длине\n",
    "    length_groups = defaultdict(list)\n",
    "    for word in set(words):\n",
    "        length_groups[len(word)].append(word)\n",
    "\n",
    "    # 4. последние 50 уникальных слов\n",
    "    last_50_unique = deque(maxlen=50)\n",
    "    seen = set()\n",
    "    for word in reversed(words):\n",
    "        if word not in seen:\n",
    "            last_50_unique.appendleft(word)\n",
    "            seen.add(word)\n",
    "\n",
    "    # 5. слова в порядке первого появления\n",
    "    first_occurrence_order = OrderedDict()\n",
    "    for idx, word in enumerate(words):\n",
    "        if word not in first_occurrence_order:\n",
    "            first_occurrence_order[word] = idx\n",
    "\n",
    "    # 6. формируем список WordInfo\n",
    "    word_info_list = []\n",
    "    for word, freq in word_counter.items():\n",
    "        word_info = WordInfo(\n",
    "            word=word,\n",
    "            frequency=freq,\n",
    "            length=len(word),\n",
    "            first_occurrence=first_occurrence_order[word]\n",
    "        )\n",
    "        word_info_list.append(word_info)\n",
    "\n",
    "    # 7. анализ памяти\n",
    "    memory_usage = {\n",
    "        'word_counter': sys.getsizeof(word_counter),\n",
    "        'length_groups': sys.getsizeof(length_groups),\n",
    "        'last_50_unique': sys.getsizeof(last_50_unique),\n",
    "        'first_occurrence_order': sys.getsizeof(first_occurrence_order),\n",
    "        'word_info_list': sys.getsizeof(word_info_list),\n",
    "        'total_words': len(words)\n",
    "    }\n",
    "\n",
    "    # 8. сохраняем результаты в JSON\n",
    "    result = {\n",
    "        'words_info': [wi._asdict() for wi in word_info_list],\n",
    "        'length_groups': {str(k): v for k, v in length_groups.items()},\n",
    "        'last_50_unique': list(last_50_unique),\n",
    "        'memory_usage': memory_usage\n",
    "    }\n",
    "\n",
    "    output_path = os.path.splitext(file_path)[0] + '_analysis.json'\n",
    "    with open(output_path, 'w', encoding='utf-8') as out_file:\n",
    "        json.dump(result, out_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"Анализ завершен. Результаты сохранены в '{output_path}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c10435",
   "metadata": {},
   "outputs": [],
   "source": [
    "#**Задача 8: Система управления задачами**\n",
    "\n",
    "import json\n",
    "import pickle\n",
    "import time\n",
    "from collections import namedtuple, defaultdict, deque, OrderedDict, Counter\n",
    "import os\n",
    "\n",
    "# 1. namedtuple для задач\n",
    "Task = namedtuple('Task', ['id', 'title', 'description', 'priority', 'status', 'created_date'])\n",
    "\n",
    "# глобальные переменные для хранения данных\n",
    "tasks = []\n",
    "next_id = 1\n",
    "# 2. группировка по статусу\n",
    "tasks_by_status = defaultdict(list)\n",
    "# 3. очередь высокого приоритета\n",
    "high_priority_queue = deque(maxlen=10)  \n",
    "# 4. счетчик приоритетов\n",
    "priority_counter = Counter()\n",
    "# 5. история по порядку создания             \n",
    "task_history = OrderedDict()             \n",
    "\n",
    "STATUS_LIST = {'todo', 'in_progress', 'done'}\n",
    "PRIORITY_LEVELS = {'low', 'medium', 'high'}\n",
    "DATA_DIR = 'task_data'\n",
    "\n",
    "def add_task(title, description, priority='medium', status='todo'):\n",
    "    \"\"\"добавление новой задачи\"\"\"\n",
    "    global next_id, tasks\n",
    "    \n",
    "    task = Task(\n",
    "        id=next_id,\n",
    "        title=title,\n",
    "        description=description,\n",
    "        priority=priority,\n",
    "        status=status,\n",
    "        created_date=time.time()\n",
    "    )\n",
    "    \n",
    "    tasks.append(task)\n",
    "    _update_collections(task)\n",
    "    next_id += 1\n",
    "    save_tasks()\n",
    "    return task\n",
    "\n",
    "def _update_collections(task):\n",
    "    \"\"\"обновление всех коллекций\"\"\"\n",
    "    tasks_by_status[task.status].append(task)\n",
    "    if task.priority == 'high':\n",
    "        high_priority_queue.appendleft(task)\n",
    "    priority_counter[task.priority] += 1\n",
    "    task_history[task.created_date] = task\n",
    "\n",
    "def complete_task(task_id):\n",
    "    \"\"\"отметка задачи как выполненной\"\"\"\n",
    "    global tasks\n",
    "    \n",
    "    for i, task in enumerate(tasks):\n",
    "        if task.id == task_id:\n",
    "            updated_task = task._replace(status='done')\n",
    "            tasks[i] = updated_task\n",
    "            _rebuild_collections()\n",
    "            save_tasks()\n",
    "            return updated_task\n",
    "    return None\n",
    "\n",
    "def _rebuild_collections():\n",
    "    \"\"\"перестроение коллекций\"\"\"\n",
    "    global tasks_by_status, high_priority_queue, priority_counter, task_history\n",
    "    \n",
    "    tasks_by_status.clear()\n",
    "    priority_counter.clear()\n",
    "    task_history.clear()\n",
    "    high_priority_queue.clear()\n",
    "    \n",
    "    for task in tasks:\n",
    "        _update_collections(task)\n",
    "\n",
    "def get_tasks_by_status(status):\n",
    "    \"\"\"получение задач по статусу\"\"\"\n",
    "    return tasks_by_status.get(status, [])\n",
    "\n",
    "def get_priority_queue():\n",
    "    \"\"\"получение очереди высокоприоритетных задач\"\"\"\n",
    "    return list(high_priority_queue)\n",
    "\n",
    "def get_tasks_by_priority(priority):\n",
    "    \"\"\"получение задач по приоритету\"\"\"\n",
    "    return [task for task in tasks if task.priority == priority]\n",
    "\n",
    "# 7. работа с файлами через os.path\n",
    "def _get_file_path(filename):\n",
    "    return os.path.join(DATA_DIR, filename)\n",
    "\n",
    "def save_tasks():\n",
    "    \"\"\"сохранение в JSON и Pickle\"\"\"\n",
    "    os.makedirs(DATA_DIR, exist_ok=True)\n",
    "    tasks_data = [task._asdict() for task in tasks]\n",
    "    \n",
    "    # JSON\n",
    "    with open(_get_file_path('tasks.json'), 'w', encoding='utf-8') as f:\n",
    "        json.dump(tasks_data, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    # Pickle\n",
    "    with open(_get_file_path('tasks.pickle'), 'wb') as f:\n",
    "        pickle.dump(tasks_data, f)\n",
    "\n",
    "def load_tasks():\n",
    "    \"\"\"загрузка из файлов\"\"\"\n",
    "    global tasks, next_id\n",
    "    \n",
    "    json_file = _get_file_path('tasks.json')\n",
    "    \n",
    "    if os.path.exists(json_file):\n",
    "        with open(json_file, 'r', encoding='utf-8') as f:\n",
    "            tasks_data = json.load(f)\n",
    "        \n",
    "        tasks = [Task(**data) for data in tasks_data]\n",
    "        next_id = max([t.id for t in tasks], default=0) + 1\n",
    "        _rebuild_collections()\n",
    "\n",
    "def display_info():\n",
    "    \"\"\"краткая информация о задачах\"\"\"\n",
    "    print(f\"\\nвсего задач: {len(tasks)}\")\n",
    "    print(\"по статусу:\", {k: len(v) for k, v in tasks_by_status.items()})\n",
    "    print(\"по приоритету:\", dict(priority_counter))\n",
    "    print(\"в очереди высокого приоритета:\", len(high_priority_queue))\n",
    "\n",
    "# 6. ChainMap для объединения различных списков задач\n",
    "def get_task_chainmap():\n",
    "    \"\"\"создание ChainMap для объединения списков задач\"\"\"\n",
    "    from collections import ChainMap\n",
    "    return ChainMap(\n",
    "        {'all_tasks': tasks},\n",
    "        {'by_status': tasks_by_status},\n",
    "        {'by_priority': {p: [t for t in tasks if t.priority == p] for p in PRIORITY_LEVELS}},\n",
    "        {'high_priority_queue': list(high_priority_queue)}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f442d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#**Задача 9: Система мониторинга производительности**\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "from collections import namedtuple, deque, defaultdict, Counter, OrderedDict\n",
    "from datetime import datetime\n",
    "\n",
    "# NamedTuple для метрик\n",
    "PerformanceMetric = namedtuple(\"PerformanceMetric\", [\"function_name\", \"execution_time\", \"memory_usage\", \"timestamp\"])\n",
    "\n",
    "# очередь для последних 100 метрик\n",
    "recent_metrics = deque(maxlen=100)\n",
    "\n",
    "# группировка по функциям\n",
    "function_metrics = defaultdict(list)\n",
    "\n",
    "# счетчик вызовов функций\n",
    "function_calls = Counter()\n",
    "\n",
    "# хронологическое хранилище\n",
    "chronological_metrics = OrderedDict()\n",
    "\n",
    "# запись метрики\n",
    "def record_metric(function_name, start_time, result):\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    memory_usage = sys.getsizeof(result)\n",
    "    timestamp = datetime.now().isoformat()\n",
    "\n",
    "    metric = PerformanceMetric(function_name, execution_time, memory_usage, timestamp)\n",
    "\n",
    "    recent_metrics.append(metric)\n",
    "    function_metrics[function_name].append(metric)\n",
    "    function_calls[function_name] += 1\n",
    "    chronological_metrics[timestamp] = metric\n",
    "\n",
    "# получение статистики по функции\n",
    "def get_function_stats(function_name):\n",
    "    metrics = function_metrics.get(function_name, [])\n",
    "    if not metrics:\n",
    "        return f\"No metrics recorded for function '{function_name}'\"\n",
    "\n",
    "    total_exec_time = sum(m.execution_time for m in metrics)\n",
    "    total_memory = sum(m.memory_usage for m in metrics)\n",
    "    count = len(metrics)\n",
    "\n",
    "    return {\n",
    "        \"function_name\": function_name,\n",
    "        \"calls\": function_calls[function_name],\n",
    "        \"avg_execution_time\": total_exec_time / count,\n",
    "        \"avg_memory_usage\": total_memory / count\n",
    "    }\n",
    "\n",
    "# получение использования памяти\n",
    "def get_memory_usage():\n",
    "    return sum(sys.getsizeof(m) for m in recent_metrics)\n",
    "\n",
    "# экспорт метрик\n",
    "def export_metrics(directory=\"metrics_output\"):\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "    # JSON\n",
    "    json_path = os.path.join(directory, \"metrics.json\")\n",
    "    with open(json_path, \"w\") as f:\n",
    "        json.dump([m._asdict() for m in recent_metrics], f, indent=4)\n",
    "\n",
    "    # Pickle\n",
    "    pickle_path = os.path.join(directory, \"metrics.pkl\")\n",
    "    with open(pickle_path, \"wb\") as f:\n",
    "        pickle.dump(list(recent_metrics), f)\n",
    "\n",
    "    print(f\"Metrics exported to {directory}/ (JSON and Pickle formats)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a2f9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#**Задача 10: Комплексная система управления данными**\n",
    "\n",
    "\n",
    "\n",
    "#Создайте комплексную систему управления данными, объединяющую все изученные концепции:\n",
    "\n",
    "#1. Создайте несколько namedtuple для различных типов данных (User, Product, Order, etc.)\n",
    "#2. Используйте `defaultdict` для создания индексов по различным полям\n",
    "#3. Используйте `deque` для реализации очередей обработки данных\n",
    "#4. Используйте `Counter` для аналитики и статистики\n",
    "#5. Используйте `OrderedDict` для хранения данных в определенном порядке\n",
    "#6. Используйте `ChainMap` для объединения различных источников данных\n",
    "#7. Используйте `os` и `sys` для работы с файловой системой и мониторинга\n",
    "#8. Реализуйте CRUD операции (Create, Read, Update, Delete)\n",
    "#9. Добавьте функции экспорта/импорта данных в различных форматах\n",
    "#10. Сериализуйте все данные в JSON, pickle и другие форматы\n",
    "#11. Добавьте типизацию для всех функций и классов\n",
    "#12. Реализуйте систему логирования для отслеживания операций"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
